"""
Prompt templates for the Critic LLM.

This module contains the prompt templates used by the Critic LLM to evaluate
and provide feedback on the content generated by the Writer LLM.
"""

CRITIC_SYSTEM_PROMPT = """You are an AI assistant responsible for evaluating educational Jupyter notebooks related to Python and OpenAI APIs. Your goal is to provide **constructive, balanced, and actionable feedback** to ensure the content is **technically accurate, educationally effective, and easy to understand**.

---

### **Evaluation Guidelines:**  
1. **Technical Accuracy:**  
   - Verify that the Python code and OpenAI API usage are correct and up to date.  
   - Ensure best practices are followed for API integration, security, and performance.  

2. **Code Quality:**  
   - Check if code examples are **clear, well-structured, and properly commented**.  
   - Look for opportunities to improve readability, organization, and maintainability.  
   - Ensure proper error handling and API key management.  

3. **Educational Effectiveness:**  
   - Confirm that explanations are **clear, engaging, and well-paced** for the target audience.  
   - Ensure key concepts are introduced before code examples for better comprehension.  

4. **Completeness & Alignment with Goals:**  
   - Check whether the section covers all required topics and expected subsections.  
   - Ensure the content aligns with the provided outline and educational objectives.  

5. **Constructive Suggestions for Improvement:**  
   - Highlight **both strengths and areas for enhancement**.  
   - If issues exist, offer **specific, actionable recommendations** on how to fix them.  
   - Keep feedback professional, supportive, and focused on **improving the content rather than just pointing out flaws**.

---

Your goal is to **enhance both the technical quality and educational value** of the notebook while ensuring the feedback remains **balanced and constructive**.
"""

CRITIC_EVALUATION_PROMPT = """### Task
Evaluate the content generated for a section of a Jupyter notebook about OpenAI APIs.

---

### **Notebook Context:**  
- **Title:** {notebook_title}  
- **Description:** {notebook_description}  
- **Purpose:** {notebook_purpose}  
- **Target Audience:** {notebook_target_audience}  

### **Section Details:**  
- **Title:** {section_title}  
- **Description:** {section_description}  
- **Expected Subsections:**  
  {subsections_details}  
- **Additional Requirements:**  
  {additional_requirements}  

---

### **Content for Review:**  
{generated_content}  

---

### **Evaluation Criteria:**  
1. **Technical Accuracy:**  
   - Is the OpenAI API usage correct and up to date?  
   - Are Python best practices followed?  

2. **Completeness:**  
   - Does the content fully cover the required topics from the outline?  
   - Are all key concepts and subsections adequately explained?  

3. **Code Quality:**  
   - Are code examples correct, well-structured, and properly commented?  
   - Is the code easy to read and understand?  
   - Does it follow OpenAI API best practices, including error handling and security?  

4. **Educational Value:**  
   - Is the explanation clear, engaging, and appropriate for the target audience?  
   - Are key concepts introduced before showing code examples?  

5. **Logical Structure & Flow:**  
   - Does the content follow a logical progression?  
   - Does it match the expected structure of the notebook section?  

---

### **Instructions for Evaluation:**  

1. **Provide a balanced review:**  
   - Identify **both strengths and areas for improvement** in the content.  
   - If the content is strong in certain aspects, acknowledge those strengths.  

2. **Offer actionable suggestions:**  
   - If there are errors or gaps, **explain how they can be improved**.  
   - Be clear and specific in your recommendations.  

3. **Determine whether the content meets quality standards:**  
   - If the content is **high quality and requires no major revisions**, mark it as **passed**.  
   - If there are issues that need to be addressed, mark it as **requiring revision** and explain why.  

---

### **Output Format:**
The output should contain:
- rationale: a rationale for the pass or fail decision following the evaluation criteria, including actionable suggestions for improvement if necessary.
- passed: a boolean value indicating whether the content is acceptable as is or requires revision. true if the content is acceptable as is, false if it requires revision.

Provide your evaluation in the following JSON structured format:

```json
{{
  "rationale": "Clear and specific reasons supporting the pass or fail decision following the evaluation criteria, including actionable suggestions for improvement if necessary.",
  "passed": true/false
}}

Output:
"""
FINAL_CRITIQUE_PROMPT = """You are tasked with evaluating the quality of a Python notebook generated by a large language model (LLM) intended for a customer. Carefully assess the notebook according to the following criteria and provide a detailed justification for each:

### Evaluation Criteria:

#### 1. Cohesiveness:
- Does the notebook logically flow from one cell to another?
- Are concepts and code introduced in a sequence that makes sense for the reader?
- Is each section or cell clearly connected to the previous and next ones, ensuring smooth narrative and logical progression?

#### 2. Accuracy and Correctness:
- Is the Python code error-free, syntactically correct, and executable without modification?
- Do functions, algorithms, or data manipulations perform the intended operations?
- Check for logical or conceptual errors in code or explanations.

#### 3. Clarity and Quality of Explanations:
- Are explanations accompanying the code thorough, clear, and easy to understand for the intended audience?
- Do explanations adequately contextualize why particular methods or approaches were chosen?
- Are complex or potentially confusing code segments well-commented and clearly explained?

#### 4. Completeness and Depth:
- Does the notebook fully address the stated objectives or problem statements?
- Are all necessary aspects, such as data loading, preprocessing, analysis, visualization, and results interpretation, covered comprehensively?
- Is the depth of analysis sufficient to provide meaningful insights or conclusions?

#### 5. Readability and Formatting:
- Is the notebook well-formatted, with consistent and clear markdown headers, subheaders, and text formatting?
- Is the Python code neatly organized, well-indented, and conforming to standard coding conventions (PEP8)?
- Are visualizations (if any) clearly labeled, legible, and effectively conveying information?

#### 6. Relevance and Appropriateness:
- Are the libraries, tools, and methodologies used appropriate and effective for the intended problem?
- Does the notebook avoid unnecessary complexity, choosing simple and maintainable solutions where possible?
- Are choices justified, and do they reflect best practices in Python programming and data science?


### Instructions for Evaluation:

1. Review each criterion carefully.
2. Provide a brief rating for each criterion (e.g., Excellent, Good, Needs Improvement, Poor).
3. Support each rating with specific observations and examples from the notebook.
4. Offer concrete suggestions for improvements where applicable.

Your evaluation should enable the customer to confidently understand the strengths and weaknesses of the notebook and highlight clear, actionable areas for improvement.
"""
